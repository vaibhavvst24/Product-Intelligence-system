# -*- coding: utf-8 -*-
"""LLM Fine-Tuning for Sentiment Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yLr4bS0AWhyURLlh_7TUmJeVN8pEXxy6
"""

import torch
torch.cuda.is_available()

!pip install transformers datasets evaluate scikit-learn rouge-score torch

"""# Install Libraries"""

import numpy as np
import torch
from datasets import load_dataset
from transformers import (
    DistilBertTokenizerFast,
    DistilBertForSequenceClassification,
    Trainer,
    TrainingArguments
)
import evaluate
from sklearn.metrics import accuracy_score, f1_score

"""# Load Dataset"""

dataset = load_dataset("imdb")

dataset

"""# Create Train / Validation Split"""

dataset = dataset["train"].train_test_split(test_size=0.1)
train_dataset = dataset["train"]
val_dataset = dataset["test"]

test_dataset = load_dataset("imdb", split="test")

"""# Tokenization Pipeline"""

tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")

"""## Define preprocessing function:"""

def tokenize_function(example):
    return tokenizer(
        example["text"],
        padding="max_length",
        truncation=True,
        max_length=256
    )

"""## Apply tokenization:"""

train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

"""## Remove unused columns:"""

train_dataset = train_dataset.remove_columns(["text"])
val_dataset = val_dataset.remove_columns(["text"])
test_dataset = test_dataset.remove_columns(["text"])

train_dataset.set_format("torch")
val_dataset.set_format("torch")
test_dataset.set_format("torch")

"""# Define Evaluation Metrics

Accuracy

F1

ROUGE
"""

accuracy_metric = evaluate.load("accuracy")
f1_metric = evaluate.load("f1")
rouge_metric = evaluate.load("rouge")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)

    acc = accuracy_metric.compute(predictions=predictions, references=labels)
    f1 = f1_metric.compute(predictions=predictions, references=labels)

    # For ROUGE we convert labels to text
    pred_text = ["positive" if p == 1 else "negative" for p in predictions]
    label_text = ["positive" if l == 1 else "negative" for l in labels]

    rouge = rouge_metric.compute(predictions=pred_text, references=label_text)

    return {
        "accuracy": acc["accuracy"],
        "f1": f1["f1"],
        "rougeL": rouge["rougeL"]
    }

"""# Load Pretrained Model (Before Fine-Tuning)"""

model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=2
)

"""# Evaluate BEFORE Fine-Tuning"""

trainer_before = Trainer(
    model=model,
    compute_metrics=compute_metrics
)

results_before = trainer_before.evaluate(test_dataset)
print("Before Fine-Tuning:", results_before)

"""# Training Arguments"""

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",          # changed from evaluation_strategy
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy"
)

"""# Trainer API (Fine-Tuning)"""

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

trainer.train()

"""# Evaluate AFTER Fine-Tuning"""

results_after = trainer.evaluate(test_dataset)
print("After Fine-Tuning:", results_after)

"""# Performance Comparison"""

print("Performance Comparison")
print("----------------------")
print("Before Fine-Tuning")
print(results_before)

print("\nAfter Fine-Tuning")
print(results_after)

"""# Save Fine-Tuned Model"""

trainer.save_model("distilbert_sentiment_model")
tokenizer.save_pretrained("distilbert_sentiment_model")

"""# Save to Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

trainer.save_model("/content/drive/MyDrive/distilbert_sentiment_model")
tokenizer.save_pretrained("/content/drive/MyDrive/distilbert_sentiment_model")

print("Model saved successfully.")
print("You can now reload without retraining.")

"""# Zip Everything and Download"""

import shutil
shutil.make_archive("distilbert_model", 'zip', "distilbert_sentiment_model")

from google.colab import files
files.download("distilbert_model.zip")

"""# Load Saved Model"""

from transformers import DistilBertForSequenceClassification
from transformers import DistilBertTokenizerFast
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model_path = "/content/drive/MyDrive/distilbert_sentiment_model"

model = DistilBertForSequenceClassification.from_pretrained(model_path)
tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)

model.to(device)
model.eval()

"""# Single Text Inference Function"""

import torch.nn.functional as F

def predict_sentiment(text):

    inputs = tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=256
    ).to(device)

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits
    probs = F.softmax(logits, dim=1)

    predicted_class = torch.argmax(probs, dim=1).item()
    confidence = probs[0][predicted_class].item()

    label_map = {0: "Negative", 1: "Positive"}

    return {
        "text": text,
        "prediction": label_map[predicted_class],
        "confidence": round(confidence, 4)
    }

"""# Test It"""

review = "This movie was absolutely fantastic. I loved the acting!"

result = predict_sentiment(review)
print(result)

"""# Batch Inference"""

def predict_batch(text_list):

    inputs = tokenizer(
        text_list,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=256
    ).to(device)

    with torch.no_grad():
        outputs = model(**inputs)

    probs = F.softmax(outputs.logits, dim=1)
    predictions = torch.argmax(probs, dim=1)

    label_map = {0: "Negative", 1: "Positive"}

    results = []

    for i, text in enumerate(text_list):
        results.append({
            "text": text,
            "prediction": label_map[predictions[i].item()],
            "confidence": round(probs[i][predictions[i]].item(), 4)
        })

    return results

reviews = [
    "Worst product ever.",
    "I really enjoyed using this item!",
    "It was okay, not great."
]

predict_batch(reviews)

import os

os.makedirs("llm_inference_package", exist_ok=True)

import shutil

shutil.copytree(
    "/content/drive/MyDrive/distilbert_sentiment_model",
    "llm_inference_package/distilbert_sentiment_model",
    dirs_exist_ok=True
)

import os
os.listdir()

import shutil

shutil.copytree(
    "/content/drive/MyDrive/distilbert_sentiment_model",
    "llm_sentiment_project/distilbert_sentiment_model",
    dirs_exist_ok=True
)

import os

os.makedirs("llm_inference_pipeline", exist_ok=True)

import shutil

shutil.make_archive("llm_inference_pipeline", 'zip', "llm_inference_pipeline")

from google.colab import files
files.download("llm_inference_pipeline.zip")

