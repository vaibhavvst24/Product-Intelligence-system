# -*- coding: utf-8 -*-
"""GAN_Product_Intelligence

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sOoXLkV6nsPrnDmlMNAaHAVXnC-Mjjrd
"""

import torch
torch.cuda.is_available()

!pip install torch torchvision matplotlib tqdm

import os

os.makedirs("gan/samples", exist_ok=True)

"""# ðŸ“¦ STEP 1 â€” Install & Import Libraries"""

!pip install torch torchvision matplotlib tqdm

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.utils import save_image, make_grid
import matplotlib.pyplot as plt
from tqdm import tqdm
import os

"""# ðŸ“ STEP 2 â€” Create Folders for Saving Outputs"""

os.makedirs("generated_images", exist_ok=True)
os.makedirs("models", exist_ok=True)
os.makedirs("plots", exist_ok=True)

"""# ðŸ§  STEP 3 â€” Hyperparameters"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

batch_size = 128
image_size = 28
latent_dim = 100
num_epochs = 50
lr = 0.0002
beta1 = 0.5

"""# ðŸ“Š STEP 4 â€” Load Dataset (Fashion MNIST)"""

transform = transforms.Compose([
    transforms.Resize(32),  # DCGAN prefers 32x32
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

dataset = torchvision.datasets.FashionMNIST(
    root="./data",
    train=True,
    transform=transform,
    download=True
)

dataloader = torch.utils.data.DataLoader(
    dataset,
    batch_size=batch_size,
    shuffle=True
)

"""# ðŸ—  STEP 5 â€” Build Generator

DCGAN-style architecture.
"""

class Generator(nn.Module):
    def __init__(self, latent_dim):
        super(Generator, self).__init__()

        self.main = nn.Sequential(
            # Input: (latent_dim, 1, 1)
            nn.ConvTranspose2d(latent_dim, 256, 4, 1, 0, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),

            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),

            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),

            nn.ConvTranspose2d(64, 1, 4, 2, 1, bias=False),
            nn.Tanh()
        )

    def forward(self, input):
        return self.main(input)

"""# ðŸ— STEP 6 â€” Build Discriminator"""

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()

        self.main = nn.Sequential(
            # Input: (1, 32, 32)
            nn.Conv2d(1, 64, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(64, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(128, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(256, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )

    def forward(self, input):
        return self.main(input).view(-1, 1)

"""# ðŸ”Œ STEP 7 â€” Initialize Models"""

generator = Generator(latent_dim).to(device)
discriminator = Discriminator().to(device)

criterion = nn.BCELoss()

optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))

"""# ðŸ”¥ STEP 8 â€” Training Loop"""

G_losses = []
D_losses = []

fixed_noise = torch.randn(64, latent_dim, 1, 1, device=device)

for epoch in range(num_epochs):
    for real_images, _ in tqdm(dataloader):

        real_images = real_images.to(device)
        batch_size_current = real_images.size(0)

        real_labels = torch.full((batch_size_current, 1), 0.9, device=device)
        fake_labels = torch.zeros(batch_size_current, 1, device=device)

        # ---------------------
        # Train Discriminator
        # ---------------------
        optimizer_D.zero_grad()

        output_real = discriminator(real_images)
        loss_real = criterion(output_real, real_labels)

        noise = torch.randn(batch_size_current, latent_dim, 1, 1, device=device)
        fake_images = generator(noise)

        output_fake = discriminator(fake_images.detach())
        loss_fake = criterion(output_fake, fake_labels)

        d_loss = loss_real + loss_fake
        d_loss.backward()
        optimizer_D.step()

        # ---------------------
        # Train Generator
        # ---------------------
        optimizer_G.zero_grad()

        output = discriminator(fake_images)
        g_loss = criterion(output, real_labels)

        g_loss.backward()
        optimizer_G.step()

    G_losses.append(g_loss.item())
    D_losses.append(d_loss.item())

    print(f"Epoch [{epoch+1}/{num_epochs}] "
          f"D Loss: {d_loss.item():.4f} "
          f"G Loss: {g_loss.item():.4f}")

    with torch.no_grad():
        fake = generator(fixed_noise)
        grid = make_grid(fake, normalize=True)
        save_image(grid, f"generated_images/epoch_{epoch+1}.png")

import os
import matplotlib.pyplot as plt
from PIL import Image

# Choose which epochs you want to show
epochs_to_show = [1, 10, 25, 50]

plt.figure(figsize=(15,5))

for i, epoch in enumerate(epochs_to_show):
    img_path = f"generated_images/epoch_{epoch}.png"

    if os.path.exists(img_path):
        img = Image.open(img_path)

        plt.subplot(1, len(epochs_to_show), i+1)
        plt.imshow(img)
        plt.title(f"Epoch {epoch}")
        plt.savefig("plots/Epoch_images.png")
        plt.axis("off")
    else:
        print(f"{img_path} not found!")

plt.tight_layout()
plt.show()

"""# ðŸ“‰ STEP 9 â€” Plot Loss Curves"""

plt.figure(figsize=(11,5), dpi=170)
plt.plot(G_losses, label="Generator Loss", color='darkblue')
plt.plot(D_losses, label="Discriminator Loss", color='crimson')
plt.legend()
plt.title("GAN Training Loss")
plt.savefig("plots/loss_curve.png")
plt.show()

"""# ðŸ’¾ STEP 10 â€” Save Model"""

torch.save(generator.state_dict(), "models/generator.pth")
torch.save(discriminator.state_dict(), "models/discriminator.pth")

"""# Evaluation FID (FrÃ©chet Inception Distance)"""

!pip install pytorch-fid

import shutil
import os

folders_to_delete = ["fid_real", "fid_fake"]

for folder in folders_to_delete:
    if os.path.exists(folder):
        shutil.rmtree(folder)
        print(f"{folder} deleted successfully.")
    else:
        print(f"{folder} does not exist.")

"""# Create Real & Fake Image Folders"""

os.makedirs("fid_real", exist_ok=True)
os.makedirs("fid_fake", exist_ok=True)

print("Fresh FID folders created.")

"""# Save Real Images"""

from torchvision.utils import save_image

real_count = 0

for images, _ in dataloader:
    for img in images:
        save_image(img, f"fid_real/{real_count}.png", normalize=True)
        real_count += 1
        if real_count >= 1000:
            break
    if real_count >= 1000:
        break

print("Saved 1000 real images.")

"""# Save Fake Images"""

generator.eval()
fake_count = 0

with torch.no_grad():
    while fake_count < 1000:
        noise = torch.randn(64, latent_dim, 1, 1).to(device)
        fake_images = generator(noise)

        for img in fake_images:
            save_image(img, f"fid_fake/{fake_count}.png", normalize=True)
            fake_count += 1
            if fake_count >= 1000:
                break

print("Saved 1000 fake DCGAN images.")

"""Now we have:

1000 real images

1000 fake images

# Compute FID
"""

!python -m pytorch_fid fid_real fid_fake --device cuda

import shutil

shutil.make_archive("gan_project", 'zip', ".")

from google.colab import files

files.download("gan_project.zip")

"""# 4.1 Training Setup

A Deep Convolutional GAN (DCGAN-inspired architecture) was trained on the Fashion MNIST dataset.
The generator receives a 100-dimensional noise vector and produces 28Ã—28 grayscale images.
The discriminator classifies images as real or synthetic.
Training was conducted for 50 epochs using Adam optimizer (lr = 0.0002, Î²1 = 0.5) with binary cross-entropy loss.

# 4.2 Visual Quality Analysis

During early epochs, generated images appeared as random noise with no discernible structure.
By epoch 20â€“30, basic clothing-like shapes began to emerge.
By the final epochs, the generator produced structured shapes resembling shirts, trousers, and footwear, indicating successful adversarial learning.

The increasing structural coherence over epochs demonstrates that the generator learned meaningful feature representations rather than memorizing patterns.

# 4.3 Loss Curve Analysis

The discriminator and generator losses exhibited oscillatory behavior, which is expected in adversarial training.
Neither loss collapsed to zero, indicating balanced competition between networks.
The absence of sudden divergence or exploding loss suggests stable training dynamics.

If the discriminator loss had rapidly approached zero, it would indicate that the generator failed to learn.
Conversely, a near-zero generator loss would suggest discriminator collapse.

# 4.4 Stability & Mode Collapse Check

Mode collapse was evaluated by visually inspecting sample diversity across epochs.
The generated images exhibited variation in clothing types and shapes, suggesting that the generator did not collapse to a single dominant pattern.

Some degree of repetition was observed, indicating partial mode collapse â€” a known issue in vanilla GAN training.

# 4.5 Business Relevance

From a product intelligence perspective, synthetic image generation can support:

Catalog augmentation for new fashion SKUs

Marketing A/B creative testing

Privacy-safe synthetic customer imagery

Low-volume product dataset expansion

# 4.6 Limitations & Improvements

Despite successful training, limitations include:

Low-resolution output (28Ã—28)

Limited realism compared to high-resolution product imagery

Potential instability in longer training runs

Improvements could include:

Transitioning to convolutional DCGAN architecture

Implementing WGAN-GP for improved stability

Training on higher-resolution datasets such as DeepFashion Dataset

Using FID score for quantitative evaluation

# 4.7 Quantitative Evaluation â€” FID Score (DCGAN)

To quantitatively evaluate the quality of generated images, the FrÃ©chet Inception Distance (FID) metric was computed between real samples from the Fashion MNIST dataset and synthetic samples generated by the DCGAN model.

A total of 1000 real and 1000 generated images were used for evaluation.

The obtained FID score was:

FID = 68.2

# Interpretation of FID Score

The FID metric measures the similarity between the distribution of real and generated images in a high-level feature space extracted using a pretrained Inception network. Lower values indicate closer alignment between the two distributions.

Compared to the earlier fully connected GAN implementation (which produced significantly higher FID), the DCGAN architecture reduced the score to 68.2, demonstrating:

Improved structural coherence in generated images

Better spatial feature learning

Increased diversity in synthetic samples

Reduced distributional mismatch

While 68.2 does not indicate near-photorealistic generation, it represents a substantial improvement and confirms that convolutional adversarial learning better captures the inherent spatial hierarchies present in image data.
"""

